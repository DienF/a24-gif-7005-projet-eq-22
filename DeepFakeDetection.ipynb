{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nofqgY-tlCBy"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KesG4h1tgvCV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import VGG16_Weights\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = pathlib.Path('/kaggle/input/deep-fake-detection-dfd-entire-original-dataset')\n",
    "original_videos = dataset_dir / \"DFD_original sequences\"\n",
    "manipulated_videos = dataset_dir / \"DFD_manipulated_sequences/DFD_manipulated_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_original_videos = len(list(original_videos.glob(\"*.mp4\")))\n",
    "num_manipulated_videos = len(list(manipulated_videos.glob(\"*.mp4\")))\n",
    "print(f\"Vidéos originelles : {num_original_videos}\")\n",
    "print(f\"Vidéos DeedFake : {num_manipulated_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/cross_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def __getitem__(self, idx):\n",
    "    label = self.labels[idx]\n",
    "    # Données d'image fictives sous forme de tableau NumPy\n",
    "    image = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)  # Simulation d'une image\n",
    "\n",
    "    if self.transform:\n",
    "        image = self.transform(Image.fromarray(image))  # Conversion en image PIL\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8xvmADaipMV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiZ_kRNSoxdN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transform=None, frames_per_video=16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.frames_per_video = frames_per_video\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Chargement de plusieurs images de la vidéo\n",
    "        frames = self.load_video_frames(video_path, self.frames_per_video)\n",
    "\n",
    "        # Application des transformations aux images\n",
    "        if self.transform:\n",
    "            frames = [self.transform(Image.fromarray(frame)) for frame in frames]\n",
    "\n",
    "        # Empilage des images pour former une séquence\n",
    "        frames = torch.stack(frames)\n",
    "        return frames, label\n",
    "\n",
    "    def load_video_frames(self, video_path, num_frames):\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Erreur lors de l'ouverture de la vidéo : {video_path}\")\n",
    "            return []  # Retourne une liste vide si la vidéo ne peut pas être ouverte\n",
    "\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=np.int32)\n",
    "\n",
    "        frames = []\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # S'il n'y a pas assez d'images, répétition de la dernière image\n",
    "        if len(frames) < num_frames:\n",
    "            frames += [frames[-1]] * (num_frames - len(frames))\n",
    "\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomAdamOptimizer:\n",
    "    def __init__(self, params, lr=0.0001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.params = list(params)\n",
    "        self.m = [torch.zeros_like(param) for param in self.params]  # Initialisation du premier moment biaisé\n",
    "        self.v = [torch.zeros_like(param) for param in self.params]  # Initialisation du second moment biaisé\n",
    "        self.t = 0  # Pas temporel\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, param in enumerate(self.params):\n",
    "            # Ignore si le paramètre ne requiert pas de gradients\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "\n",
    "            grad = param.grad  # Obtention du gradient du paramètre\n",
    "\n",
    "            # Mise à jour du premier moment biaisé\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            # Mise à jour du second moment biaisé\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2\n",
    "\n",
    "            # Calcul du premier moment corrigé\n",
    "            m_hat = self.m[i] / (1 - self.beta1**self.t)\n",
    "            # Calcul du second moment corrigé\n",
    "            v_hat = self.v[i] / (1 - self.beta2**self.t)\n",
    "\n",
    "            # Mise à jour du paramètre avec la règle Adam\n",
    "            param.data = param.data - self.lr * m_hat / (torch.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def balance_dataset(original_videos_dir, manipulated_videos_dir, output_dir, target_count=75):\n",
    "\n",
    "    # Crée des répertoires de sortie\n",
    "    balanced_original_dir = output_dir / \"original\"\n",
    "    balanced_manipulated_dir = output_dir / \"manipulated\"\n",
    "    balanced_original_dir.mkdir(parents=True, exist_ok=True)\n",
    "    balanced_manipulated_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Échantillonnage les vidéos\n",
    "    original_videos = list(original_videos_dir.glob(\"*.mp4\"))\n",
    "    manipulated_videos = list(manipulated_videos_dir.glob(\"*.mp4\"))\n",
    "\n",
    "    sampled_original = random.sample(original_videos, target_count)\n",
    "    sampled_manipulated = random.sample(manipulated_videos, target_count)\n",
    "\n",
    "    # Copie les vidéos échantillonnées dans les répertoires de sortie\n",
    "    for file in sampled_original:\n",
    "        shutil.copy(file, balanced_original_dir / file.name)\n",
    "\n",
    "    for file in sampled_manipulated:\n",
    "        shutil.copy(file, balanced_manipulated_dir / file.name)\n",
    "\n",
    "    print(f\"Jeu de données équilibré créé avec {target_count} vidéos dans chaque classe.\")\n",
    "    return (sampled_original, [0] * len(sampled_original)), (sampled_manipulated, [1] * len(sampled_manipulated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_original_videos = len(list(original_videos.glob(\"*.mp4\")))  # Modification de l'extension du fichier si nécessaire\n",
    "num_manipulated_videos = len(list(manipulated_videos.glob(\"*.mp4\")))\n",
    "\n",
    "print(f\"Vidéos originelles : {num_original_videos}\")\n",
    "print(f\"Vidéos DeedFake : {num_manipulated_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvBBSyHuiOhn",
    "outputId": "b4280ecf-168f-437a-ca31-36f85b140898",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Chemins\n",
    "balanced_dir = pathlib.Path('./balanced_dataset')\n",
    "balanced_samples = balance_dataset(original_videos, manipulated_videos, balanced_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "balanced_dir = pathlib.Path('./balanced_dataset')\n",
    "balanced_original = balanced_dir / \"original\"\n",
    "balanced_manipulated = balanced_dir / \"manipulated\"\n",
    "\n",
    "# S'assure que les répertoires de sortie existent\n",
    "balanced_original.mkdir(parents=True, exist_ok=True)\n",
    "balanced_manipulated.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLU56CwFS_Wo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Mise à jour des chemins du jeu de données pour pointer vers le nouveau sous-ensemble\n",
    "balanced_video_paths = []\n",
    "balanced_labels = []\n",
    "\n",
    "for video_path in balanced_original.glob(\"*.mp4\"):\n",
    "    balanced_video_paths.append(video_path)\n",
    "    balanced_labels.append(0)  # Étiquette 0 pour les vidéos originelles\n",
    "\n",
    "for video_path in balanced_manipulated.glob(\"*.mp4\"):\n",
    "    balanced_video_paths.append(video_path)\n",
    "    balanced_labels.append(1)  # Étiquette 1 pour les vidéos DeepFake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Dataset équilibré créé avec 350 vidéos dans chaque classe.\")\n",
    "print(f\"Vidéos originelles enregistrées dans : {balanced_original}\")\n",
    "print(f\"Vidéos DeepFake enregistrées dans : {balanced_manipulated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "balanced_original_dir = pathlib.Path('./balanced_dataset/original')\n",
    "balanced_manipulated_dir = pathlib.Path('./balanced_dataset/manipulated')\n",
    "\n",
    "original_count = len(list(balanced_original_dir.glob(\"*.mp4\")))\n",
    "manipulated_count = len(list(balanced_manipulated_dir.glob(\"*.mp4\")))\n",
    "print(f\"Nombre de vidéos originelles : {original_count}\")\n",
    "print(f\"Nombre de vidéos DeepFake : {manipulated_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "balanced_dataset = VideoDataset(balanced_video_paths,balanced_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtrJhtz2isFd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Validation croisée en 3 plis\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "fold_splits = [(train_idx, val_idx) for train_idx, val_idx in kf.split(balanced_video_paths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W3VHuQXgzub"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "class VGG16LSTM(nn.Module):\n",
    "    def __init__(self, num_classes=2, lstm_hidden_size=256, lstm_num_layers=1, freeze_feature_extractor=True):\n",
    "        super(VGG16LSTM, self).__init__()\n",
    "\n",
    "        # Charger les caractéristiques du VGG16 pré-entraîné\n",
    "        self.feature_extractor = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # VGG16 utilise par défaut un pool (7x7)\n",
    "        self.fc_features = nn.Linear(512 * 7 * 7, 1024)\n",
    "\n",
    "        # Optionnel : geler l'extracteur de caractéristiques\n",
    "        if freeze_feature_extractor:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # LSTM pour la modélisation temporelle\n",
    "        self.lstm = nn.LSTM(input_size=1024, hidden_size=lstm_hidden_size, num_layers=lstm_num_layers, batch_first=True)\n",
    "\n",
    "        # Couche entièrement connectée pour la classification\n",
    "        self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "\n",
    "        # Redimensionner l'entrée pour l'extracteur de caractéristiques\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # Pooling et aplatissage\n",
    "        features = self.avgpool(features)\n",
    "        features = torch.flatten(features, start_dim=1)\n",
    "        features = self.fc_features(features)\n",
    "\n",
    "        # Redimensionnement pour l'entrée du LSTM\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # Aplatissage des poids du LSTM (nécessaire pour CuDNN)\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        # Passage avant à travers le LSTM\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "\n",
    "        # Prend la sortie du dernier pas temporel\n",
    "        final_output = lstm_out[:, -1, :]\n",
    "\n",
    "        # Passage avant à travers le classifieur\n",
    "        output = self.fc(final_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73GLybBoi-A4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Barre de progression\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialisation du modèle, de la fonction de perte et de l'optimiseur\n",
    "model = VGG16LSTM(num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = CustomAdamOptimizer(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Entraînement et validation\n",
    "def train_and_validate(model, train_loader, val_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Passe le modèle en mode entraînement\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Boucle d'entraînement avec barre de progression\n",
    "        print(f\"Époque {epoch + 1}/{epochs}\")\n",
    "        train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Entraînement\")\n",
    "\n",
    "        for batch_idx, (videos, labels) in train_progress:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "            # Passage avant\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Rétropropagation et mise à jour de l'optimiseur\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Époque {epoch + 1} Perte d'entraînement : {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Boucle de validation\n",
    "        model.eval()  # Passe le modèle en mode évaluation\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_progress = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\")\n",
    "            for batch_idx, (videos, labels) in val_progress:\n",
    "                videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "                # Passage avant\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calcul de la précision\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        print(f\"Époque {epoch + 1} Perte de validation : {val_loss / len(val_loader):.4f}\")\n",
    "        print(f\"Époque {epoch + 1} Précision de validation : {100 * correct / total:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import  precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    # Collecte des prédictions et des vraies étiquettes\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in val_loader:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            outputs = model(videos)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            tp += ((preds == 1) & (labels == 1)).sum().item()\n",
    "            tn += ((preds == 0) & (labels == 0)).sum().item()\n",
    "            fp += ((preds == 1) & (labels == 0)).sum().item()\n",
    "            fn += ((preds == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "    # Calcul des métriques\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    cm = np.array([[tp, fn], [fp, tn]])\n",
    "\n",
    "    return cm, acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMfEl1bQr3yk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"exactitude\": [],\n",
    "    \"précision\": [],\n",
    "    \"rappel\": [],\n",
    "    \"score_f1\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Division du jeu de données\n",
    "train_idx = list(range(0, 120))  # Ajustement selon le jeu de données\n",
    "val_idx = list(range(120, 150))\n",
    "train_dataset = Subset(balanced_dataset, train_idx)\n",
    "val_dataset = Subset(balanced_dataset, val_idx)\n",
    "# Chargeurs de données (DataLoaders)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vérifie si ces répertoires contiennent des fichiers vidéo\n",
    "print(len(list(original_videos.glob(\"*.mp4\"))))\n",
    "print(len(list(manipulated_videos.glob(\"*.mp4\"))))\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, labels=[\"Positif\", \"Négatif\"]):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[\"Positif\", \"Négatif\"],\n",
    "                yticklabels=[\"Positif\", \"Négatif\"])\n",
    "    plt.ylabel(\"Valeurs prédites\")\n",
    "    plt.xlabel(\"Valeurs réelles\")\n",
    "    plt.title(\"Matrice de confusion\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RpBmPtVjROV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Boucle de validation croisée\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(fold_splits):\n",
    "    print(f\"\\nPli {fold_idx + 1} :\")\n",
    "    # Préparation des DataLoaders pour le pli\n",
    "    train_dataset = Subset(balanced_dataset, train_idx)\n",
    "    val_dataset = Subset(balanced_dataset, val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    # Initialisation du modèle et de l'optimiseur pour chaque pli\n",
    "    model = VGG16LSTM(num_classes=2).to(device)\n",
    "    model.lstm.flatten_parameters()\n",
    "    optimizer = CustomAdamOptimizer(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Entraînement et validation pour le pli\n",
    "    train_and_validate(model, train_loader, val_loader, epochs=10)\n",
    "    # Évaluation des métriques finales pour le pli\n",
    "    cm, acc, precision, recall, f1 = evaluate_model(model, val_loader)\n",
    "    plot_confusion_matrix(cm)\n",
    "    print(f\"Métriques finales pour le pli {fold_idx + 1} - Exactitude : {acc:.4f}, Précision: {precision:.4f}, Rappel : {recall:.4f}, Score F1: {f1:.4f}\\n\")\n",
    "    # Évaluation après pli\n",
    "    writer.add_scalar(f\"Pli_{fold_idx+1}/Exactitude\", acc, fold_idx + 1)\n",
    "    writer.add_scalar(f\"Pli_{fold_idx+1}/Précision\", precision, fold_idx + 1)\n",
    "    writer.add_scalar(f\"Pli_{fold_idx+1}/Rappel\", recall, fold_idx + 1)\n",
    "    writer.add_scalar(f\"Pli_{fold_idx+1}/Score_F1\", f1, fold_idx + 1)\n",
    "    writer.add_figure(f\"Pli_{fold_idx+1}/Matrice_Confusion\", plt.gcf(), fold_idx + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/kaggle/working/vgg16_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/cross_validation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5524489,
     "sourceId": 9146200,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
