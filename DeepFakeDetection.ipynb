{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nofqgY-tlCBy"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KesG4h1tgvCV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from typing import Callable, Iterable, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialisation des générateurs de nombres aléatoires pour garantir la reproductibilité\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Répertoire des vidéos originales et manipulées\n",
    "dataset_dir: Path = pathlib.Path('/kaggle/input/deep-fake-detection-dfd-entire-original-dataset')\n",
    "original_videos: Path = dataset_dir / \"DFD_original sequences\"\n",
    "manipulated_videos: Path = dataset_dir / \"DFD_manipulated_sequences/DFD_manipulated_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Comptage des vidéos dans chaque répertoire\n",
    "num_original_videos: int = len(list(original_videos.glob(\"*.mp4\")))\n",
    "num_manipulated_videos: int = len(list(manipulated_videos.glob(\"*.mp4\")))\n",
    "print(f\"Vidéos originelles : {num_original_videos}\")\n",
    "print(f\"Vidéos DeedFake : {num_manipulated_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Initialisation du SummaryWriter pour TensorBoard\n",
    "writer: SummaryWriter = SummaryWriter(log_dir=\"runs/cross_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def __getitem__(self, idx: int) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Obtention d'un élément du jeu de données\n",
    "\n",
    "    Récupère une image et son étiquette associée à l'indice spécifié.\n",
    "    Les images sont simulées comme des tableaux NumPy. Une transformation peut être appliquée si définie.\n",
    "\n",
    "    Paramètre(s)\n",
    "    ----------\n",
    "    idx : int\n",
    "        Indice de l'élément à récupérer.\n",
    "\n",
    "    Retourne\n",
    "    -------\n",
    "    Tuple[np.ndarray, int]\n",
    "        Un tuple contenant l'image (après transformation si applicable) et l'étiquette associée.\n",
    "    \"\"\"\n",
    "    label: int = self.labels[idx]\n",
    "    # Données d'image fictives sous forme de tableau NumPy\n",
    "    image: np.ndarray = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)  # Simulation d'une image\n",
    "    if self.transform:\n",
    "        image = self.transform(Image.fromarray(image))  # Conversion en image PIL\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8xvmADaipMV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Transformations pour l'image (redimensionnement, normalisation, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiZ_kRNSoxdN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Jeu de données pour charger des vidéos et leurs étiquettes associées\n",
    "\n",
    "    Cette classe gère le chargement des images de vidéos, l'application\n",
    "    d'éventuelles transformations, et la préparation des séquences pour\n",
    "    des modèles d'apprentissage profond.\n",
    "\n",
    "    Paramètre(s)\n",
    "    ----------\n",
    "    video_paths : List[Path]\n",
    "        Liste des chemins des fichiers vidéo.\n",
    "    labels : List[int]\n",
    "        Liste des étiquettes correspondant à chaque vidéo.\n",
    "    transform : Optional[Callable], optional\n",
    "        Transformation à appliquer aux images, par défaut None.\n",
    "    frames_per_video : int, optional\n",
    "        Nombre d'images à charger par vidéo, par défaut 16.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_paths: List[Path], labels: List[int], transform: Optional[Callable] = None, frames_per_video: int = 16) -> None:\n",
    "        self.video_paths: List[Path] = video_paths\n",
    "        self.labels: List[int] = labels\n",
    "        self.transform: Optional[Callable] = transform\n",
    "        self.frames_per_video: int = frames_per_video\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Retourne la taille du jeu de données.\"\"\"\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Retourne une séquence d'images et l'étiquette associée.\n",
    "\n",
    "        Paramètre(s)\n",
    "        ----------\n",
    "        idx : int\n",
    "            Indice de l'échantillon à récupérer.\n",
    "\n",
    "        Retourne\n",
    "        -------\n",
    "        Tuple[torch.Tensor, int]\n",
    "            Séquence d'images sous forme de tenseur PyTorch et étiquette associée.\n",
    "        \"\"\"\n",
    "        video_path: Path = self.video_paths[idx]\n",
    "        label: int = self.labels[idx]\n",
    "        # Chargement de plusieurs images de la vidéo\n",
    "        frames: List[np.ndarray] = self.load_video_frames(video_path, self.frames_per_video)\n",
    "        # Application des transformations aux images\n",
    "        if self.transform:\n",
    "            frames = [self.transform(Image.fromarray(frame)) for frame in frames]\n",
    "        # Empilage des images pour former une séquence\n",
    "        frames_tensor: torch.Tensor = torch.stack(frames)\n",
    "        return frames_tensor, label\n",
    "\n",
    "    def load_video_frames(self, video_path: Path, num_frames: int) -> List[np.ndarray]:\n",
    "        \"\"\"Charge un nombre spécifique d'images d'une vidéo.\n",
    "\n",
    "        Paramètre(s)\n",
    "        ----------\n",
    "        video_path : Path\n",
    "            Chemin de la vidéo.\n",
    "        num_frames : int\n",
    "            Nombre de images à charger.\n",
    "\n",
    "        Retourne\n",
    "        -------\n",
    "        List[np.ndarray]\n",
    "            Liste des images sous forme de tableaux NumPy (en RGB).\n",
    "        \"\"\"\n",
    "        cap: cv2.VideoCapture = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Erreur lors de l'ouverture de la vidéo : {video_path}\")\n",
    "            return []  # Retourne une liste vide si la vidéo ne peut pas être ouverte\n",
    "        frame_count: int = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices: np.ndarray = np.linspace(0, frame_count - 1, num_frames, dtype=np.int32)\n",
    "        frames: List[np.ndarray] = []\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb: np.ndarray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "            else:\n",
    "                break\n",
    "        cap.release()\n",
    "        # S'il n'y a pas assez d'images, répétition de la dernière image\n",
    "        if len(frames) < num_frames:\n",
    "            frames += [frames[-1]] * (num_frames - len(frames))\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomAdamOptimizer:\n",
    "    \"\"\"Implémentation personnalisée de l'optimiseur Adam\n",
    "\n",
    "    Cet optimiseur applique la méthode d'Adam pour mettre à jour les paramètres\n",
    "    d'un modèle avec un apprentissage adaptatif basé sur les moments biaisés.\n",
    "\n",
    "    Paramètre(s)\n",
    "    ----------\n",
    "    params : Iterable[Parameter]\n",
    "        Liste des paramètres à optimiser.\n",
    "    lr : float, optional\n",
    "        Taux d'apprentissage, par défaut 0.0001.\n",
    "    beta1 : float, optional\n",
    "        Coefficient pour le premier moment biaisé, par défaut 0.9.\n",
    "    beta2 : float, optional\n",
    "        Coefficient pour le second moment biaisé, par défaut 0.999.\n",
    "    epsilon : float, optional\n",
    "        Petite constante pour éviter la division par zéro, par défaut 1e-8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[Parameter], lr: float = 0.0001, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8) -> None:\n",
    "        self.lr: float = lr\n",
    "        self.beta1: float = beta1\n",
    "        self.beta2: float = beta2\n",
    "        self.epsilon: float = epsilon\n",
    "        self.params: List[Parameter] = list(params)\n",
    "        self.m: List[torch.Tensor] = [torch.zeros_like(param) for param in self.params]  # Initialisation du premier moment biaisé\n",
    "        self.v: List[torch.Tensor] = [torch.zeros_like(param) for param in self.params]  # Initialisation du second moment biaisé\n",
    "        self.t: int = 0  # Pas temporel\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"Met à jour les paramètres en utilisant la méthode Adam.\"\"\"\n",
    "        self.t += 1\n",
    "        for i, param in enumerate(self.params):\n",
    "            # Ignore si le paramètre ne requiert pas de gradients\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            grad: torch.Tensor = param.grad  # Obtention du gradient du paramètre\n",
    "            # Mise à jour du premier moment biaisé\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            # Mise à jour du second moment biaisé\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
    "            # Calcul du premier moment corrigé\n",
    "            m_hat: torch.Tensor = self.m[i] / (1 - self.beta1**self.t)\n",
    "            # Calcul du second moment corrigé\n",
    "            v_hat: torch.Tensor = self.v[i] / (1 - self.beta2**self.t)\n",
    "            # Mise à jour du paramètre avec la méthode d'Adam\n",
    "            param.data = param.data - self.lr * m_hat / (torch.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Réinitialise tous les gradients des paramètres suivis.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def balance_dataset(original_videos_dir: Path, manipulated_videos_dir: Path, output_dir: Path, target_count: int = 75) -> Tuple[Tuple[List[Path], List[int]], Tuple[List[Path], List[int]]]:\n",
    "    \"\"\"Équilibrage du jeu de données\n",
    "\n",
    "    Équilibre un jeu de données en sélectionnant un nombre égal de vidéos originales\n",
    "    et DeepFake, puis les copie dans un répertoire de sortie.\n",
    "\n",
    "    Paramètre(s)\n",
    "    ----------\n",
    "    original_videos_dir : Path\n",
    "        Répertoire contenant les vidéos originales.\n",
    "    manipulated_videos_dir : Path\n",
    "        Répertoire contenant les vidéos DeepFake.\n",
    "    output_dir : Path\n",
    "        Répertoire où les vidéos équilibrées seront sauvegardées.\n",
    "    target_count : int, optional\n",
    "        Nombre de vidéos à sélectionner pour chaque classe, par défaut 75.\n",
    "\n",
    "    Retourne\n",
    "    -------\n",
    "    Tuple[Tuple[List[Path], List[int]], Tuple[List[Path], List[int]]]\n",
    "        Une paire de tuples contenant les listes de chemins des vidéos sélectionnées\n",
    "        et leurs étiquettes correspondantes (0 pour originales, 1 pour DeepFake).\n",
    "    \"\"\"\n",
    "    # Création des répertoires de sortie\n",
    "    balanced_original_dir: Path = output_dir / \"original\"\n",
    "    balanced_manipulated_dir: Path = output_dir / \"manipulated\"\n",
    "    balanced_original_dir.mkdir(parents=True, exist_ok=True)\n",
    "    balanced_manipulated_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Échantillonnage des vidéos\n",
    "    original_videos: List[Path] = list(original_videos_dir.glob(\"*.mp4\"))\n",
    "    manipulated_videos: List[Path] = list(manipulated_videos_dir.glob(\"*.mp4\"))\n",
    "    sampled_original: List[Path] = random.sample(original_videos, target_count)\n",
    "    sampled_manipulated: List[Path] = random.sample(manipulated_videos, target_count)\n",
    "    # Copie des vidéos échantillonnées dans les répertoires de sortie\n",
    "    for file in sampled_original:\n",
    "        shutil.copy(file, balanced_original_dir / file.name)\n",
    "    for file in sampled_manipulated:\n",
    "        shutil.copy(file, balanced_manipulated_dir / file.name)\n",
    "    print(f\"Jeu de données équilibré créé avec {target_count} vidéos dans chaque classe.\")\n",
    "    return (sampled_original, [0] * len(sampled_original)), (sampled_manipulated, [1] * len(sampled_manipulated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Nouvelle vérification du nombre de vidéos dans les répertoires\n",
    "num_original_videos = len(list(original_videos.glob(\"*.mp4\")))  # Modification de l'extension du fichier si nécessaire\n",
    "num_manipulated_videos = len(list(manipulated_videos.glob(\"*.mp4\")))\n",
    "# Affichage du nombre de vidéos par catégorie\n",
    "print(f\"Vidéos originelles : {num_original_videos}\")\n",
    "print(f\"Vidéos DeedFake : {num_manipulated_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvBBSyHuiOhn",
    "outputId": "b4280ecf-168f-437a-ca31-36f85b140898",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Création des répertoires pour les vidéos équilibrées\n",
    "balanced_dir: Path = pathlib.Path('./balanced_dataset')\n",
    "balanced_samples: Tuple[Tuple[List[Path], List[int]], Tuple[List[Path], List[int]]] = balance_dataset(original_videos, manipulated_videos, balanced_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Redéfinition des chemins pour les vidéos équilibrées\n",
    "balanced_dir = pathlib.Path('./balanced_dataset')\n",
    "balanced_original: Path = balanced_dir / \"original\"\n",
    "balanced_manipulated: Path = balanced_dir / \"manipulated\"\n",
    "# S'assure que les répertoires de sortie existent\n",
    "balanced_original.mkdir(parents=True, exist_ok=True)\n",
    "balanced_manipulated.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLU56CwFS_Wo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Mise à jour des chemins du jeu de données pour pointer vers le nouveau sous-ensemble\n",
    "balanced_video_paths: List[Path] = []\n",
    "balanced_labels: List[int] = []\n",
    "for video_path in balanced_original.glob(\"*.mp4\"):\n",
    "    balanced_video_paths.append(video_path)\n",
    "    balanced_labels.append(0)  # Étiquette 0 pour les vidéos originelles\n",
    "for video_path in balanced_manipulated.glob(\"*.mp4\"):\n",
    "    balanced_video_paths.append(video_path)\n",
    "    balanced_labels.append(1)  # Étiquette 1 pour les vidéos DeepFake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Affichage de l'état du jeu de données équilibré\n",
    "print(f\"Dataset équilibré créé avec 350 vidéos dans chaque classe.\")\n",
    "print(f\"Vidéos originelles enregistrées dans : {balanced_original}\")\n",
    "print(f\"Vidéos DeepFake enregistrées dans : {balanced_manipulated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vérification des vidéos après équilibrage\n",
    "balanced_original_dir: Path = pathlib.Path('./balanced_dataset/original')\n",
    "balanced_manipulated_dir: Path = pathlib.Path('./balanced_dataset/manipulated')\n",
    "original_count = len(list(balanced_original_dir.glob(\"*.mp4\")))\n",
    "manipulated_count = len(list(balanced_manipulated_dir.glob(\"*.mp4\")))\n",
    "print(f\"Nombre de vidéos originelles : {original_count}\")\n",
    "print(f\"Nombre de vidéos DeepFake : {manipulated_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialisation du jeu de données avec les vidéos et leurs étiquettes\n",
    "balanced_dataset: VideoDataset = VideoDataset(balanced_video_paths, balanced_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtrJhtz2isFd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Validation croisée en 3 plis\n",
    "kf: KFold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "fold_splits: List[Tuple[np.ndarray, np.ndarray]] = [(train_idx, val_idx) for train_idx, val_idx in kf.split(balanced_video_paths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W3VHuQXgzub"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "class VGG16LSTM(nn.Module):\n",
    "    \"\"\"Combinaison d'un extracteur de caractéristiques basé sur VGG16 et d'un LSTM pour la classification vidéo\n",
    "\n",
    "    Cette architecture utilise VGG16 pour extraire les caractéristiques spatiales de chaque image,\n",
    "    suivie d'un LSTM pour capturer les dépendances temporelles dans une séquence d'images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int, optional\n",
    "        Nombre de classes pour la tâche de classification, par défaut 2.\n",
    "    lstm_hidden_size : int, optional\n",
    "        Taille de la couche cachée du LSTM, par défaut 256.\n",
    "    lstm_num_layers : int, optional\n",
    "        Nombre de couches dans le LSTM, par défaut 1.\n",
    "    freeze_feature_extractor : bool, optional\n",
    "        Si True, gèle les poids de l'extracteur de caractéristiques VGG16, par défaut True.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    feature_extractor : nn.Module\n",
    "        Extracteur de caractéristiques basé sur VGG16.\n",
    "    avgpool : nn.AdaptiveAvgPool2d\n",
    "        Pooling adaptatif pour réduire les dimensions des caractéristiques.\n",
    "    fc_features : nn.Linear\n",
    "        Couche linéaire pour réduire les dimensions des caractéristiques avant le LSTM.\n",
    "    lstm : nn.LSTM\n",
    "        Réseau LSTM pour capturer les dépendances temporelles.\n",
    "    fc : nn.Linear\n",
    "        Couche linéaire pour la classification finale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 2, lstm_hidden_size: int = 256, lstm_num_layers: int = 1, freeze_feature_extractor: bool = True) -> None:\n",
    "        super(VGG16LSTM, self).__init__()\n",
    "        # Chargement des caractéristiques du VGG16 pré-entraîné\n",
    "        self.feature_extractor: nn.Module = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n",
    "        self.avgpool: nn.AdaptiveAvgPool2d = nn.AdaptiveAvgPool2d((7, 7))  # VGG16 utilise par défaut un pool (7x7)\n",
    "        self.fc_features: nn.Linear = nn.Linear(512 * 7 * 7, 1024)\n",
    "        # Optionnel : geler l'extracteur de caractéristiques\n",
    "        if freeze_feature_extractor:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        # LSTM pour la modélisation temporelle\n",
    "        self.lstm: nn.LSTM = nn.LSTM(\n",
    "            input_size=1024,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # Couche entièrement connectée pour la classification\n",
    "        self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Passe avant du modèle\n",
    "\n",
    "        Paramètre(s)\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Tenseur de forme (batch_size, seq_len, c, h, w), où :\n",
    "            - batch_size : Taille du lot\n",
    "            - seq_len : Longueur de la séquence temporelle\n",
    "            - c : Nombre de canaux (typiquement 3 pour RGB)\n",
    "            - h : Hauteur de l'image\n",
    "            - w : Largeur de l'image\n",
    "\n",
    "        Retourne\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Prédictions de forme (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        # Redimensionnement de l'entrée pour l'extracteur de caractéristiques\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "        features = self.feature_extractor(x)\n",
    "        # Pooling et aplatissage\n",
    "        features = self.avgpool(features)\n",
    "        features = torch.flatten(features, start_dim=1)\n",
    "        features = self.fc_features(features)\n",
    "        # Redimensionnement pour l'entrée du LSTM\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "        # Aplatissage des poids du LSTM (nécessaire pour CuDNN)\n",
    "        self.lstm.flatten_parameters()\n",
    "        # Passage avant à travers le LSTM\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        # Prend la sortie du dernier pas temporel\n",
    "        final_output: torch.Tensor = lstm_out[:, -1, :]\n",
    "        # Passage avant à travers le classifieur\n",
    "        output: torch.Tensor = self.fc(final_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73GLybBoi-A4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Détection du périphérique pour l'entraînement (GPU si disponible)\n",
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialisation du modèle, de la fonction de perte et de l'optimiseur\n",
    "model: VGG16LSTM = VGG16LSTM(num_classes=2).to(device)\n",
    "criterion: nn.CrossEntropyLoss = nn.CrossEntropyLoss()\n",
    "optimizer: CustomAdamOptimizer = CustomAdamOptimizer(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_and_validate(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, epochs: int) -> None:\n",
    "    \"\"\"Entraînement et validation d'un modèle PyTorch sur plusieurs époques\n",
    "\n",
    "    Cette fonction alterne entre les étapes d'entraînement et de validation à chaque époque,\n",
    "    en affichant les pertes et la précision à la fin de chaque phase.\n",
    "\n",
    "    Paramètre(s)\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Le modèle PyTorch à entraîner.\n",
    "    train_loader : DataLoader\n",
    "        DataLoader pour l'ensemble d'entraînement.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader pour l'ensemble de validation.\n",
    "    epochs : int\n",
    "        Nombre d'époques pour l'entraînement.\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Passe le modèle en mode entraînement\n",
    "        running_loss: float = 0.0\n",
    "        # Boucle d'entraînement avec barre de progression\n",
    "        print(f\"Époque {epoch + 1}/{epochs}\")\n",
    "        train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Entraînement\")\n",
    "        for batch_idx, (videos, labels) in train_progress:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            # Passage avant\n",
    "            optimizer.zero_grad()\n",
    "            outputs: torch.Tensor = model(videos)\n",
    "            loss: torch.Tensor = criterion(outputs, labels)\n",
    "            # Rétropropagation et mise à jour de l'optimiseur\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix(loss=loss.item())\n",
    "        avg_train_loss: float = running_loss / len(train_loader)\n",
    "        print(f\"Époque {epoch + 1} Perte d'entraînement : {avg_train_loss:.4f}\")\n",
    "        # Boucle de validation\n",
    "        model.eval()  # Passe le modèle en mode évaluation\n",
    "        val_loss: float = 0.0\n",
    "        correct: int = 0\n",
    "        total: int = 0\n",
    "        with torch.no_grad():\n",
    "            val_progress = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\")\n",
    "            for batch_idx, (videos, labels) in val_progress:\n",
    "                videos, labels = videos.to(device), labels.to(device)\n",
    "                # Passage avant\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                # Calcul de la précision\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        avg_val_loss: float = val_loss / len(val_loader)\n",
    "        val_accuracy: float = 100 * correct / total\n",
    "        print(f\"Époque {epoch + 1} Perte de validation : {avg_val_loss:.4f}\")\n",
    "        print(f\"Époque {epoch + 1} Précision de validation : {val_accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import  precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, val_loader: DataLoader) -> Tuple[np.ndarray, float, float, float, float]:\n",
    "    \"\"\"Évaluation d'un modèle PyTorch sur un ensemble de validation\n",
    "\n",
    "    Cette fonction calcule les métriques de classification (matrice de confusion, précision,\n",
    "    rappel, score F1) en collectant les prédictions et les étiquettes réelles.\n",
    "\n",
    "    Paramètre(s)\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Le modèle PyTorch à évaluer.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader pour l'ensemble de validation.\n",
    "\n",
    "    Retourne\n",
    "    -------\n",
    "    Tuple[np.ndarray, float, float, float, float]\n",
    "        - Matrice de confusion (2x2) sous forme de tableau NumPy.\n",
    "        - Précision globale (accuracy).\n",
    "        - Précision pondérée (precision).\n",
    "        - Rappel pondéré (recall).\n",
    "        - Score F1 pondéré (f1_score).\n",
    "    \"\"\"\n",
    "    y_true: List[int] = []\n",
    "    y_pred: List[int] = []\n",
    "    tp: int = 0  # Vrais positifs\n",
    "    tn: int = 0  # Vrais négatifs\n",
    "    fp: int = 0  # Faux positifs\n",
    "    fn: int = 0  # Faux négatifs\n",
    "    # Collecte des prédictions et des vraies étiquettes\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in val_loader:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            outputs: torch.Tensor = model(videos)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            tp += ((preds == 1) & (labels == 1)).sum().item()\n",
    "            tn += ((preds == 0) & (labels == 0)).sum().item()\n",
    "            fp += ((preds == 1) & (labels == 0)).sum().item()\n",
    "            fn += ((preds == 0) & (labels == 1)).sum().item()\n",
    "    # Calcul des métriques\n",
    "    acc: float = accuracy_score(y_true, y_pred)\n",
    "    precision: float = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    recall: float = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    f1: float = f1_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    cm: np.ndarray = np.array([[tp, fn], [fp, tn]])\n",
    "    return cm, acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMfEl1bQr3yk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les métriques des différents plis\n",
    "metrics: dict[str, List[float]] = {\n",
    "    \"exactitude\": [],\n",
    "    \"précision\": [],\n",
    "    \"rappel\": [],\n",
    "    \"score_f1\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sélection des indices pour l'entraînement et la validation\n",
    "train_idx: List[int] = list(range(0, 120))  # Ajustement selon le jeu de données\n",
    "val_idx: List[int] = list(range(120, 150))\n",
    "train_dataset: Subset = Subset(balanced_dataset, train_idx)\n",
    "val_dataset: Subset = Subset(balanced_dataset, val_idx)\n",
    "# Initialisation des DataLoaders\n",
    "train_loader: DataLoader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader: DataLoader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vérifie si ces répertoires contiennent des fichiers vidéo\n",
    "print(len(list(original_videos.glob(\"*.mp4\"))))\n",
    "print(len(list(manipulated_videos.glob(\"*.mp4\"))))\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm: np.ndarray, labels: List[str] = [\"Positif\", \"Négatif\"]) -> None:\n",
    "    \"\"\"Affichage d'une matrice de confusion sous forme de carte thermique (heatmap)\n",
    "\n",
    "    Cette fonction prend une matrice de confusion sous forme de tableau NumPy\n",
    "    et la visualise en utilisant `seaborn` et `matplotlib`.\n",
    "\n",
    "    Paramètre(s)\n",
    "    ----------\n",
    "    cm : np.ndarray\n",
    "        Matrice de confusion (2x2) sous forme de tableau NumPy.\n",
    "    labels : List[str], optional\n",
    "        Liste des étiquettes à afficher pour les axes x et y. Par défaut, [\"Positif\", \"Négatif\"].\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels\n",
    "    )\n",
    "    plt.ylabel(\"Valeurs prédites\")\n",
    "    plt.xlabel(\"Valeurs réelles\")\n",
    "    plt.title(\"Matrice de confusion\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RpBmPtVjROV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Boucle de validation croisée\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(fold_splits):\n",
    "    print(f\"\\nPli {fold_idx + 1} :\")\n",
    "    # Préparation des DataLoaders pour le pli\n",
    "    train_dataset = Subset(balanced_dataset, train_idx)\n",
    "    val_dataset = Subset(balanced_dataset, val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    # Initialisation du modèle et de l'optimiseur pour chaque pli\n",
    "    model = VGG16LSTM(num_classes=2).to(device)\n",
    "    model.lstm.flatten_parameters()\n",
    "    optimizer = CustomAdamOptimizer(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Entraînement et validation pour le pli\n",
    "    train_and_validate(model, train_loader, val_loader, epochs=10)\n",
    "    # Évaluation des métriques finales pour le pli\n",
    "    cm, acc, precision, recall, f1 = evaluate_model(model, val_loader)\n",
    "    plot_confusion_matrix(cm)\n",
    "    # Affichage des métriques du pli\n",
    "    print(f\"Métriques finales pour le pli {fold_idx + 1} - Exactitude : {acc:.4f}, Précision: {precision:.4f}, Rappel : {recall:.4f}, Score F1: {f1:.4f}\\n\")\n",
    "    # Enregistrement des métriques dans TensorBoard\n",
    "    writer.add_scalar(f\"Pli_{fold_idx+1}/Exactitude\", acc, fold_idx + 1)\n",
    "    writer.add_scalar(f\"Pli_{fold_idx+1}/Précision\", precision, fold_idx + 1)\n",
    "    writer.add_scalar(f\"Pli_{fold_idx+1}/Rappel\", recall, fold_idx + 1)\n",
    "    writer.add_scalar(f\"Pli_{fold_idx+1}/Score_F1\", f1, fold_idx + 1)\n",
    "    writer.add_figure(f\"Pli_{fold_idx+1}/Matrice_Confusion\", plt.gcf(), fold_idx + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sauvegarde du meilleur modèle\n",
    "torch.save(model.state_dict(), '/kaggle/working/vgg16_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fermeture de TensorBoard\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lancement de TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/cross_validation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5524489,
     "sourceId": 9146200,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
