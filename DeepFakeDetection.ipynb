{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":9146200,"sourceType":"datasetVersion","datasetId":5524489}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Pre-processing","metadata":{"id":"nofqgY-tlCBy"}},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import transforms, models\nfrom torchvision.models import VGG16_Weights\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nimport torch.nn.functional as F\nfrom skopt import gp_minimize\nfrom skopt.space import Real\nfrom torch import nn\nfrom torch.optim import Adam\nimport cv2\nimport shutil\nimport numpy as np\nfrom PIL import Image\nimport random\nimport json","metadata":{"id":"KesG4h1tgvCV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_dir = pathlib.Path('/kaggle/input/deep-fake-detection-dfd-entire-original-dataset')\noriginal_videos = dataset_dir / \"DFD_original sequences\"\nmanipulated_videos = dataset_dir / \"DFD_manipulated_sequences/DFD_manipulated_sequences\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_original_videos = len(list(original_videos.glob(\"*.mp4\"))) \nnum_manipulated_videos = len(list(manipulated_videos.glob(\"*.mp4\")))\nprint(f\"Original videos: {num_original_videos}\")\nprint(f\"Manipulated videos: {num_manipulated_videos}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter(log_dir=\"runs/cross_validation\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper functions","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\n\ndef __getitem__(self, idx):\n    label = self.labels[idx]\n    # Mock image data as a NumPy array\n    image = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)  # Simulating an image\n    \n    if self.transform:\n        image = self.transform(Image.fromarray(image))  # Convert to PIL Image\n\n    return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),  \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"id":"-8xvmADaipMV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    def __init__(self, video_paths, labels, transform=None, frames_per_video=16):\n        self.video_paths = video_paths\n        self.labels = labels\n        self.transform = transform\n        self.frames_per_video = frames_per_video\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        label = self.labels[idx]\n\n        # Load multiple frames from the video\n        frames = self.load_video_frames(video_path, self.frames_per_video)\n\n        # Apply transformations to frames\n        if self.transform:\n            frames = [self.transform(Image.fromarray(frame)) for frame in frames]\n        \n        # Stack frames to form a sequence\n        frames = torch.stack(frames)\n        return frames, label\n\n\n    def load_video_frames(self, video_path, num_frames):\n        cap = cv2.VideoCapture(str(video_path))\n        if not cap.isOpened():\n            print(f\"Error opening video: {video_path}\")\n            return []  # Return empty if the video cannot be opened\n\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=np.int32)\n    \n        frames = []\n        for idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frames.append(frame)\n            else:\n                break\n        cap.release()\n    \n        # If we don't have enough frames, repeat the last frame\n        if len(frames) < num_frames:\n            frames += [frames[-1]] * (num_frames - len(frames))\n    \n        return frames\n","metadata":{"id":"hiZ_kRNSoxdN","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomAdamOptimizer:\n    def __init__(self, params, lr=0.0001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        \n        self.params = list(params)\n        self.m = [torch.zeros_like(param) for param in self.params]  # Initialize first moment estimate\n        self.v = [torch.zeros_like(param) for param in self.params]  # Initialize second moment estimate\n        self.t = 0  # Time step\n        \n    def step(self):\n        self.t += 1\n        for i, param in enumerate(self.params):\n            # Skip if the parameter does not require gradients\n            if not param.requires_grad:\n                continue\n            \n            grad = param.grad  # Get the gradient of the parameter\n            \n            # Update biased first moment estimate\n            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n            # Update biased second moment estimate\n            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2\n            \n            # Compute bias-corrected first moment estimate\n            m_hat = self.m[i] / (1 - self.beta1**self.t)\n            # Compute bias-corrected second moment estimate\n            v_hat = self.v[i] / (1 - self.beta2**self.t)\n            \n            # Update the parameter using the Adam rule\n            param.data = param.data - self.lr * m_hat / (torch.sqrt(v_hat) + self.epsilon)\n\n    def zero_grad(self):\n        for param in self.params:\n            if param.grad is not None:\n                param.grad.zero_()","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def balance_dataset(original_videos_dir, manipulated_videos_dir, output_dir, target_count=75):\n\n    # Create output directories\n    balanced_original_dir = output_dir / \"original\"\n    balanced_manipulated_dir = output_dir / \"manipulated\"\n    balanced_original_dir.mkdir(parents=True, exist_ok=True)\n    balanced_manipulated_dir.mkdir(parents=True, exist_ok=True)\n\n    # Sample videos\n    original_videos = list(original_videos_dir.glob(\"*.mp4\"))\n    manipulated_videos = list(manipulated_videos_dir.glob(\"*.mp4\"))\n\n    sampled_original = random.sample(original_videos, target_count)\n    sampled_manipulated = random.sample(manipulated_videos, target_count)\n\n    # Copy sampled videos to output directories\n    for file in sampled_original:\n        shutil.copy(file, balanced_original_dir / file.name)\n\n    for file in sampled_manipulated:\n        shutil.copy(file, balanced_manipulated_dir / file.name)\n\n    print(f\"Balanced dataset created with {target_count} videos in each class.\")\n    return (sampled_original, [0] * len(sampled_original)), (sampled_manipulated, [1] * len(sampled_manipulated))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_original_videos = len(list(original_videos.glob(\"*.mp4\")))  # Adjust file extension if needed\nnum_manipulated_videos = len(list(manipulated_videos.glob(\"*.mp4\")))\n\nprint(f\"Original videos: {num_original_videos}\")\nprint(f\"Manipulated videos: {num_manipulated_videos}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths\nbalanced_dir = pathlib.Path('./balanced_dataset')\nbalanced_samples = balance_dataset(original_videos, manipulated_videos, balanced_dir)","metadata":{"id":"FvBBSyHuiOhn","outputId":"b4280ecf-168f-437a-ca31-36f85b140898","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"balanced_dir = pathlib.Path('./balanced_dataset')\nbalanced_original = balanced_dir / \"original\"\nbalanced_manipulated = balanced_dir / \"manipulated\"\n\n# Ensure the output directories exist\nbalanced_original.mkdir(parents=True, exist_ok=True)\nbalanced_manipulated.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update dataset paths to point to the new subset\nbalanced_video_paths = []\nbalanced_labels = []\n\nfor video_path in balanced_original.glob(\"*.mp4\"):\n    balanced_video_paths.append(video_path)\n    balanced_labels.append(0)  # Label 0 for original\n\nfor video_path in balanced_manipulated.glob(\"*.mp4\"):\n    balanced_video_paths.append(video_path)\n    balanced_labels.append(1)  # Label 1 for manipulated","metadata":{"id":"qLU56CwFS_Wo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Balanced dataset created with 350 videos in each class.\")\nprint(f\"Original videos saved to: {balanced_original}\")\nprint(f\"Manipulated videos saved to: {balanced_manipulated}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"balanced_original_dir = pathlib.Path('./balanced_dataset/original')\nbalanced_manipulated_dir = pathlib.Path('./balanced_dataset/manipulated')\n\noriginal_count = len(list(balanced_original_dir.glob(\"*.mp4\")))\nmanipulated_count = len(list(balanced_manipulated_dir.glob(\"*.mp4\")))\nprint(f\"Number of videos in 'original': {original_count}\")\nprint(f\"Number of videos in 'manipulated': {manipulated_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"balanced_dataset = VideoDataset(balanced_video_paths,balanced_labels, transform=transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3-Fold Cross Validation\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nfold_splits = [(train_idx, val_idx) for train_idx, val_idx in kf.split(balanced_video_paths)]","metadata":{"id":"CtrJhtz2isFd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{"id":"5W3VHuQXgzub"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchvision.models import VGG16_Weights\n\nclass VGG16LSTM(nn.Module):\n    def __init__(self, num_classes=2, lstm_hidden_size=256, lstm_num_layers=1, freeze_feature_extractor=True):\n        super(VGG16LSTM, self).__init__()\n        \n        # Load pretrained VGG16 features\n        self.feature_extractor = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # VGG16 uses a (7x7) pool by default\n        self.fc_features = nn.Linear(512 * 7 * 7, 1024)\n\n        # Optionally freeze the feature extractor\n        if freeze_feature_extractor:\n            for param in self.feature_extractor.parameters():\n                param.requires_grad = False\n\n        # LSTM for temporal modeling\n        self.lstm = nn.LSTM(input_size=1024, hidden_size=lstm_hidden_size, num_layers=lstm_num_layers, batch_first=True)\n        \n        # Fully connected layer for classification\n        self.fc = nn.Linear(lstm_hidden_size, num_classes)\n\n    def forward(self, x):\n        batch_size, seq_len, c, h, w = x.size()\n        \n        # Reshape input for the feature extractor\n        x = x.view(batch_size * seq_len, c, h, w)\n        features = self.feature_extractor(x)\n        \n        # Pool and flatten\n        features = self.avgpool(features)\n        features = torch.flatten(features, start_dim=1)\n        features = self.fc_features(features)\n        \n        # Reshape for LSTM input\n        features = features.view(batch_size, seq_len, -1)\n        \n        # Flatten LSTM weights (needed for CuDNN)\n        self.lstm.flatten_parameters()\n        \n        # Forward through LSTM\n        lstm_out, _ = self.lstm(features)\n        \n        # Take the last time step's output\n        final_output = lstm_out[:, -1, :]\n        \n        # Forward through the classifier\n        output = self.fc(final_output)\n        return output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm  # Progress bar\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Initialize model, loss function, and optimizer\nmodel = VGG16LSTM(num_classes=2).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = CustomAdamOptimizer(model.parameters(), lr=0.0001)","metadata":{"id":"73GLybBoi-A4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and validate\ndef train_and_validate(model, train_loader, val_loader, epochs):\n    for epoch in range(epochs):\n        model.train()  # Sezt model to training mode\n        running_loss = 0.0\n\n        # Training loop with progress bar\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n\n        for batch_idx, (videos, labels) in train_progress:\n            videos, labels = videos.to(device), labels.to(device)\n\n            # Forward pass\n            optimizer.zero_grad()\n            outputs = model(videos)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimizer step\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            train_progress.set_postfix(loss=loss.item())\n\n        print(f\"Epoch {epoch + 1} Training Loss: {running_loss / len(train_loader):.4f}\")\n\n        # Validation loop\n        model.eval()  # Set model to evaluation mode\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            val_progress = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validating\")\n            for batch_idx, (videos, labels) in val_progress:\n                videos, labels = videos.to(device), labels.to(device)\n\n                # Forward pass\n                outputs = model(videos)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                # Accuracy calculation\n                _, preds = torch.max(outputs, 1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n\n        print(f\"Epoch {epoch + 1} Validation Loss: {val_loss / len(val_loader):.4f}\")\n        print(f\"Epoch {epoch + 1} Validation Accuracy: {100 * correct / total:.2f}%\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import  precision_score, recall_score, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef evaluate_model(model, val_loader):\n    y_true = []\n    y_pred = []\n    tp=0\n    tn=0\n    fp=0\n    fn=0\n\n\n    # Collect predictions and true labels\n    model.eval()\n    with torch.no_grad():\n        for videos, labels in val_loader:\n            videos, labels = videos.to(device), labels.to(device)\n            outputs = model(videos)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n            tp += ((preds == 1) & (labels == 1)).sum().item()\n            tn += ((preds == 0) & (labels == 0)).sum().item()\n            fp += ((preds == 1) & (labels == 0)).sum().item()\n            fn += ((preds == 0) & (labels == 1)).sum().item()\n\n\n    # Compute metrics\n    \n    acc = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n    recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=1)\n    cm= np.array([[tp, fn], [fp, tn]])\n\n    return cm,acc, precision, recall, f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics = {\n    \"accuracy\": [],\n    \"preision\": [],\n    \"recall\": [],\n    \"f1_score\": []\n}","metadata":{"id":"iMfEl1bQr3yk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset split\ntrain_idx = list(range(0, 120))  # Adjust according to your dataset\nval_idx = list(range(120, 150))\ntrain_dataset = Subset(balanced_dataset, train_idx)\nval_dataset = Subset(balanced_dataset, val_idx)\n# Dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if these directories contain video files\nprint(len(list(original_videos.glob(\"*.mp4\"))))\nprint(len(list(manipulated_videos.glob(\"*.mp4\"))))\nprint(len(train_dataset))\nprint(len(val_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(cm, labels=[\"Positive\", \"Negative\"]):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=[\"Positive\", \"Negative\"], \n                yticklabels=[\"Positive\", \"Negative\"])\n    plt.ylabel(\"Predicted Values\")\n    plt.xlabel(\"Actual Values\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cross-validation loop\nfor fold_idx, (train_idx, val_idx) in enumerate(fold_splits):\n    print(f\"\\nFold {fold_idx + 1}:\")\n\n    # Prepare DataLoaders for the fold\n    train_dataset = Subset(balanced_dataset, train_idx)\n    val_dataset = Subset(balanced_dataset, val_idx)\n    \n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n\n    # Initialize model and optimizer for each fold\n    model = VGG16LSTM(num_classes=2).to(device)\n    model.lstm.flatten_parameters()\n    optimizer = CustomAdamOptimizer(model.parameters(), lr=0.0001)\n    criterion = nn.CrossEntropyLoss()\n\n    # Train and validate for the fold\n    train_and_validate(model, train_loader, val_loader, epochs=10)\n\n    \n    # Evaluate final metrics for the fold\n    cm, acc, precision, recall, f1 = evaluate_model(model, val_loader)\n    plot_confusion_matrix(cm)\n    \n    print(f\"Final Fold {fold_idx + 1} Metrics - Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\\n\")\n    # After fold evaluation\n    writer.add_scalar(f\"Fold_{fold_idx+1}/Accuracy\", acc, fold_idx + 1)\n    writer.add_scalar(f\"Fold_{fold_idx+1}/Precision\", precision, fold_idx + 1)\n    writer.add_scalar(f\"Fold_{fold_idx+1}/Recall\", recall, fold_idx + 1)\n    writer.add_scalar(f\"Fold_{fold_idx+1}/F1_Score\", f1, fold_idx + 1)\n    writer.add_figure(f\"Fold_{fold_idx+1}/Confusion_Matrix\", plt.gcf(), fold_idx + 1)\n    ","metadata":{"id":"3RpBmPtVjROV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/vgg16_best_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"writer.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir runs/cross_validation","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}